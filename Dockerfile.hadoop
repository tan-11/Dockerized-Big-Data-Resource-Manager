# Start from a standard Ubuntu 20.04 image
FROM ubuntu:focal

# Set an argument for easier updates
ARG HADOOP_VERSION=3.3.1
ARG SPARK_VERSION=3.2.1
ARG KAFKA_VERSION=3.1.0

# Avoid interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive


# Install all necessary dependencies:
# - openssh-server & sudo: For SSH access, same as before.
# - openssh-client: To allow SSH to connect Hadoop nodes
# - wget: To download Hadoop.
# - openjdk-8-jdk: Java is required to run Hadoop.
RUN echo 'Acquire::http::Pipeline-Depth "0";' > /etc/apt/apt.conf.d/99fixbadproxy && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* && \
    apt-get update --fix-missing&& \
    apt-get install -y --no-install-recommends \
        openssh-server \
        openssh-client \
        sudo \
        wget \
        openjdk-8-jdk \
    && rm -rf /var/lib/apt/lists/*



# === User and SSH Key Setup ===
RUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers

# The Hadoop start-dfs.sh script require passwordless SSH access
RUN ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa && \
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && \
    chmod 600 /root/.ssh/authorized_keys

# === SSH Configuration ===
RUN mkdir /var/run/sshd && \
    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config && \
    sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin no/' /etc/ssh/sshd_config

# === Java Environment Setup ===
# Set the JAVA_HOME environment variable, which Hadoop needs to find Java.
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# Explicitly allow the root user to run Hadoop services.
ENV HDFS_NAMENODE_USER="root"
ENV HDFS_DATANODE_USER="root"
ENV HDFS_SECONDARYNAMENODE_USER="root"

# === Hadoop Installation ===
# Copy the pre-downloaded Hadoop file from our project directory into the container.
COPY hadoop-${HADOOP_VERSION}.tar.gz /tmp/hadoop.tar.gz

# Unpack it, move it to /opt/, and clean up.
RUN tar -xzf /tmp/hadoop.tar.gz && \
    mv hadoop-${HADOOP_VERSION} /opt/hadoop && \
    rm /tmp/hadoop.tar.gz

# Set Hadoop environment variables so you can run its commands easily.
ENV HADOOP_HOME=/opt/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# === Spark Installation ===
# Copy the pre-downloaded Spark file from our project directory into the container.
COPY spark-${SPARK_VERSION}-bin-hadoop3.2.tgz /tmp/spark.tgz

# Unpack it, move it to /opt/, and clean up.
RUN tar -xzf /tmp/spark.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop3.2 /opt/spark && \
    rm /tmp/spark.tgz

# Set Spark environment variables.
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# It tells Spark to use the same config files as Hadoop, so it automatically knows where HDFS is.
ENV SPARK_CONF_DIR=$HADOOP_HOME/etc/hadoop

# === Kafka Installation (depend on ZooKeeper) ===
COPY kafka_2.13-${KAFKA_VERSION}.tgz /tmp/kafka.tgz

RUN tar -xzf /tmp/kafka.tgz && \
    mv kafka_2.13-${KAFKA_VERSION} /opt/kafka && \
    rm /tmp/kafka.tgz

# Set Kafka environment variables.    
ENV KAFKA_HOME=/opt/kafka
ENV PATH=$PATH:$KAFKA_HOME/bin

# Create a system-wide environment file for all users.(all env and their PATHs)
RUN echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' > /etc/profile.d/all_env.sh
RUN echo 'export HADOOP_HOME=/opt/hadoop' >> /etc/profile.d/all_env.sh
RUN echo 'export SPARK_HOME=/opt/spark' >> /etc/profile.d/all_env.sh 
RUN echo 'export KAFKA_HOME=/opt/kafka' >> /etc/profile.d/all_env.sh
RUN echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$KAFKA_HOME/bin' >> /etc/profile.d/all_env.sh

# === Hadoop Configuration for a Single-Node Cluster ===
# These files are required for HDFS (Hadoop's file system) to run.
# 1. core-site.xml: Tells Hadoop where the main NameNode is running.
RUN echo '<configuration><property><name>fs.defaultFS</name><value>hdfs://localhost:9000</value></property></configuration>' > $HADOOP_HOME/etc/hadoop/core-site.xml

# 2. hdfs-site.xml: Sets the replication factor to 1 (since we only have one node).
RUN echo '<configuration><property><name>dfs.replication</name><value>1</value></property></configuration>' > $HADOOP_HOME/etc/hadoop/hdfs-site.xml

# Set JAVA_HOME inside Hadoop's own environment script.
# This is the official way to ensure Hadoop scripts know where Java is, especially for SSH-based startup.
RUN echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# 3. CRITICAL: Format the HDFS NameNode. This initializes the file system.
# This must be done once during the image build.
RUN $HADOOP_HOME/bin/hdfs namenode -format

# Expose the ports: 22 for SSH and 9000 for the HDFS NameNode, 4040 for Spark UI, 8080 for Spark Master UI,
# 2181 for Zookeeper, and 9092 for Kafka Broker.
EXPOSE 22 9000 4040 8080 2181 9092

# === Startup Script Setup ===
COPY entrypoint.sh /entrypoint.sh

# FIX: Remove Windows carriage returns (\r) from the script
RUN sed -i 's/\r$//' /entrypoint.sh

RUN chmod +x /entrypoint.sh

# Set our script as the entrypoint. It will run before the main command.
ENTRYPOINT ["/entrypoint.sh"]
